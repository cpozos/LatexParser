# Papers

[1][What you get is what you see: A visual markup decompiler](https://arxiv.org/pdf/1609.04938v1.pdf)

[2][Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

# Latex generator

[1] [Image-to-Markup Generation with Coarse-to-Fine Attention
Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, Alexander M. Rush](http://lstm.seas.harvard.edu/latex/)

[2] [Translating Math Formula Images to LaTeX Sequences Using Deep
Neural Networks with Sequence-level Training](https://arxiv.org/pdf/1908.11415.pdf)

[3] [Seq2Seq for LaTeX generation](https://guillaumegenthial.github.io/image-to-latex.html)

[4] [Image To Latex with DenseNet Encoder and Joint Attention](https://www.sciencedirect.com/science/article/pii/S1877050919302686)

[5] [Im2Latex](https://github.com/luopeixiang/im2latex)

## CNN

[1] [CNN Standford class](https://cs231n.github.io/)

## NLP

[1] [NLP Standford class](https://web.stanford.edu/class/cs224n/)

[2] [Batching](https://www.youtube.com/watch?v=U4WB9p6ODjM)

### Sequence2Sequence

[1] [Seq2Seq with Attention and Beam Search](https://guillaumegenthial.github.io/sequence-to-sequence.html)

### Encoder-decoder

[1] [A Convolutional Encoder Model for Neural Machine Translation, Jonas Gehring, Michael Auli, David Grangier, Yann N. Dauphin](https://research.fb.com/wp-content/uploads/2017/11/convolutional-encoder-model-45.pdf)

[2] Natural Language Processing with PyTorch, Delip Rao & Brian McMahan, Chapter 8
